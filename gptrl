import gymnasium as gym
import numpy as np

def get_status(_observation):
    # RAMベースの観測データを状態として直接使用
    return _observation

def update_q_table(_q_table, _action, _observation, _next_observation, _reward, alpha=0.2, gamma=0.99):
    # 状態の取得
    state = get_status(_observation)
    next_state = get_status(_next_observation)

    # 最大Q値の取得
    next_max_q_value = max(_q_table[next_state])

    # 現在のQ値の更新
    q_value = _q_table[state][_action]
    _q_table[state][_action] = q_value + alpha * (_reward + gamma * next_max_q_value - q_value)

    return _q_table

def get_action(_q_table, _observation, epsilon=0.02):
    state = get_status(_observation)
    if np.random.uniform(0, 1) > epsilon:
        _action = np.argmax(_q_table[state])
    else:
        _action = np.random.choice(18)  # 行動空間のサイズ
    return _action

def main():
    env = gym.make("ALE/Boxing-v5", render_mode='human', obs_type="ram")

    # Qテーブルの初期化（RAMのサイズと行動空間を考慮）
    q_table = np.zeros((256, 256, 256, 256, 18))  # 例：4バイトのRAMデータ用

    # ゲーム環境を初期化
    observation, info = env.reset()

    # ゲームのステップをプレイ
    for episode in range(1000):
        action = get_action(q_table, observation)
        next_observation, reward, terminated, truncated, info = env.step(action)

        # Qテーブルの更新
        q_table = update_q_table(q_table, action, observation, next_observation, reward)

        observation = next_observation

        # ゲームが終了したら、環境を初期化して再開
        if terminated or truncated:
            observation, info = env.reset()

    env.close()

if __name__ == '__main__':
    main()
